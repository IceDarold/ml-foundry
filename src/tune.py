# src/tune.py

import hydra
from omegaconf import DictConfig, OmegaConf
import optuna
from optuna_integration import WandbCallback
import wandb

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–∞—à—É –æ—Å–Ω–æ–≤–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –æ–±—É—á–µ–Ω–∏—è
from src.train import train


# –°–æ–∑–¥–∞–µ–º W&B callback –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
wandb_callback = WandbCallback(
    metric_name="oof_score_mean", # –ú–µ—Ç—Ä–∏–∫–∞, –∫–æ—Ç–æ—Ä—É—é Optuna –±—É–¥–µ—Ç –º–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å
    wandb_kwargs={"project": "tuning-project-name"} # –£–∫–∞–∂–∏—Ç–µ –≤–∞—à –ø—Ä–æ–µ–∫—Ç –¥–ª—è —Ç—é–Ω–∏–Ω–≥–∞
)

# ==================================================================================
# Objective Function
# ==================================================================================
@wandb_callback.track_in_wandb() # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ª–æ–≥–∏—Ä—É–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
def objective(trial: optuna.trial.Trial, cfg: DictConfig) -> float:
    """
    "–¶–µ–ª–µ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è" –¥–ª—è Optuna.

    Optuna –≤—ã–∑—ã–≤–∞–µ—Ç —ç—Ç—É —Ñ—É–Ω–∫—Ü–∏—é –Ω–∞ –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏, –ø–µ—Ä–µ–¥–∞–≤–∞—è –æ–±—ä–µ–∫—Ç `trial`,
    –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
    –§—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–µ—Ç—Ä–∏–∫—É –∫–∞—á–µ—Å—Ç–≤–∞.
    """
    
    # 1. –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º –Ω–æ–≤—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    # –ú—ã –±—É–¥–µ–º –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞ –≤ YAML-–∫–æ–Ω—Ñ–∏–≥–µ
    # –∏ "–∏–Ω–∂–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å" –µ–≥–æ –≤ `cfg` –ø–µ—Ä–µ–¥ –≤—ã–∑–æ–≤–æ–º.
    # Optuna –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏—è, –∞ –º—ã –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º –∏–º–∏ –∫–æ–Ω—Ñ–∏–≥.
    
    # –ü—Ä–∏–º–µ—Ä:
    # cfg.model.params.learning_rate = trial.suggest_float('learning_rate', 0.01, 0.1, log=True)
    # cfg.model.params.max_depth = trial.suggest_int('max_depth', 3, 10)
    
    # --- –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ –∫–æ–Ω—Ñ–∏–≥ ---
    for param, search_space in cfg.tuning.search_space.items():
        # param –±—É–¥–µ—Ç, –Ω–∞–ø—Ä–∏–º–µ—Ä, 'model.params.learning_rate'
        # search_space - —Å–ª–æ–≤–∞—Ä—å {'type': 'float', 'low': 0.01, 'high': 0.1, 'log': True}
        
        param_type = search_space.pop('type')
        
        if param_type == 'float':
            value = trial.suggest_float(param.split('.')[-1], **search_space)
        elif param_type == 'int':
            value = trial.suggest_int(param.split('.')[-1], **search_space)
        elif param_type == 'categorical':
            value = trial.suggest_categorical(param.split('.')[-1], **search_space)
        else:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –ø–∞—Ä–∞–º–µ—Ç—Ä–∞: {param_type}")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –æ–±—ä–µ–∫—Ç–µ –∫–æ–Ω—Ñ–∏–≥–∞ —Å –ø–æ–º–æ—â—å—é OmegaConf
        OmegaConf.update(cfg, param, value)
    
    # 2. –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –Ω–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    # –ú—ã –≤—ã–∑—ã–≤–∞–µ–º –Ω–∞—à—É –æ—Å–Ω–æ–≤–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é `train`, –∫–æ—Ç–æ—Ä–∞—è –≤–µ—Ä–Ω–µ—Ç oof_score.
    # W&B –±—É–¥–µ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –≤–Ω—É—Ç—Ä–∏ `train`, –Ω–æ callback –µ–≥–æ "–ø–æ–¥—Ö–≤–∞—Ç–∏—Ç".
    try:
        oof_score = train(cfg)
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è trial: {e}. Optuna –æ–±—Ä–∞–±–æ—Ç–∞–µ—Ç —ç—Ç–æ –∫–∞–∫ –Ω–µ—É–¥–∞—á–Ω—ã–π –∑–∞–ø—É—Å–∫.")
        # –ï—Å–ª–∏ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∏–∑-–∑–∞ –Ω–µ—Ö–≤–∞—Ç–∫–∏ –ø–∞–º—è—Ç–∏),
        # Optuna –¥–æ–ª–∂–Ω–∞ –∑–Ω–∞—Ç—å, —á—Ç–æ —ç—Ç–æ—Ç trial –ø—Ä–æ–≤–∞–ª–∏–ª—Å—è.
        raise optuna.exceptions.TrialPruned()
        
    return oof_score

# ==================================================================================
# Main Tuning Function
# ==================================================================================
@hydra.main(config_path="../conf", config_name="config", version_base=None)
def tune(cfg: DictConfig) -> None:
    """
    –ì–ª–∞–≤–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–æ–¥–±–æ—Ä–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
    """
    
    print("--- –ó–∞–ø—É—Å–∫ –ø–æ–¥–±–æ—Ä–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ ---")
    print(OmegaConf.to_yaml(cfg.tuning))
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –∏–º—è –ø—Ä–æ–µ–∫—Ç–∞ –≤ W&B callback –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
    wandb_callback.wandb_kwargs['project'] = cfg.wandb.project
    wandb_callback.wandb_kwargs['entity'] = cfg.wandb.entity

    # –°–æ–∑–¥–∞–µ–º "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ" Optuna
    study = optuna.create_study(
        direction=cfg.tuning.direction, # "maximize" –∏–ª–∏ "minimize"
        study_name=f"{cfg.model._target_.split('.')[-1]}-{cfg.features.name}-tuning"
    )

    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é
    study.optimize(
        lambda trial: objective(trial, cfg),
        n_trials=cfg.tuning.n_trials,
        callbacks=[wandb_callback]
    )

    print("\n--- –ü–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω ---")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö trials: {len(study.trials)}")
    print("–õ—É—á—à–∏–π trial:")
    trial = study.best_trial

    print(f"  Value (OOF Score): {trial.value}")
    print("  Params: ")
    for key, value in trial.params.items():
        print(f"    {key}: {value}")
        
    print("\nüí° –°–∫–æ–ø–∏—Ä—É–π—Ç–µ —ç—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ –≤–∞—à `conf/model/*.yaml` –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è 'tuned' –≤–µ—Ä—Å–∏–∏ –º–æ–¥–µ–ª–∏.")

if __name__ == "__main__":
    tune()