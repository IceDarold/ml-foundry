# Advanced stacking experiment with meta-learner
# Uses LightGBM as meta-learner on top of multiple base models

name: "titanic_stacking_v1"

# Base models for stacking
base_models:
  - name: "lgbm_baseline"
    oof_path: "05_oof/oof_exp001_baseline_lgbm.csv"
    test_preds_path: "07_submissions/submission_exp001_baseline_lgbm.csv"
  - name: "catboost_baseline"
    oof_path: "05_oof/oof_exp002_baseline_catboost.csv"
    test_preds_path: "07_submissions/submission_exp002_baseline_catboost.csv"
  - name: "lgbm_tuned"
    oof_path: "05_oof/oof_exp003_tuned_lgbm.csv"
    test_preds_path: "07_submissions/submission_exp003_tuned_lgbm.csv"
  - name: "feature_engineering"
    oof_path: "05_oof/oof_exp004_feature_engineering.csv"
    test_preds_path: "07_submissions/submission_exp004_feature_engineering.csv"

# Meta-learner configuration
meta_model:
  _target_: src.models.lgbm.LGBMModel
  params:
    objective: "binary"
    metric: "auc"
    n_estimators: 100
    learning_rate: 0.1
    max_depth: 3
    num_leaves: 7
    random_state: ${globals.seed}

# W&B tags for experiment tracking
wandb:
  tags: ["stacking", "meta_learner", "lgbm", "exp006"]